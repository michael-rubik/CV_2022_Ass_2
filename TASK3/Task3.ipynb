{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3 - Scene recognition with Bag of Visual Words\n",
    "\n",
    "Group Number: #INSERT HERE\n",
    "\n",
    "\n",
    "### 1 Data import and Preparation\n",
    "\n",
    "Complete the class ***SceneDataset*** in ***dataset.py*** and load the training and test dataset. (images and class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport features, dataset, utils\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import features\n",
    "import dataset\n",
    "import utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported test data:  400\n",
      "Imported training data:  1600\n",
      "['forest', 'bedroom', 'office', 'coast', 'tallbuilding', 'street', 'livingroom', 'mountain', 'kitchen', 'store']\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(os.getcwd(), 'results')):\n",
    "    os.makedirs(os.path.join(os.getcwd(), 'results'))\n",
    "\n",
    "#student_code start\n",
    "\n",
    "# Init datasets\n",
    "train_path = \"./data/scene/train/\"\n",
    "test_path =  \"./data/scene/test/\"\n",
    "train_dataset = dataset.SceneDataset(train_path)\n",
    "test_dataset = dataset.SceneDataset(test_path)\n",
    "\n",
    "# Get images and class names\n",
    "train_images = train_dataset.get_data()[0]\n",
    "test_images = test_dataset.get_data()[0]\n",
    "class_names = test_dataset.get_class_names()[0]\n",
    "\n",
    "#student_code end\n",
    "\n",
    "print('Imported test data: ',len(test_images))\n",
    "print('Imported training data: ', len(train_images))\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2 Build Vocabulary and Clusters\n",
    "\n",
    "Extract SIFT features from the training data using ***features.extract_dsift(..)*** and then utilize the descriptors to generate visual words with ***KMeans(...).fit(...).cluster_centers_*** from ***sklearn.neighbors***. Set the sample size per image at about **100** and a step size around **5**. For cluster size, try a value around **50**. You can adapt the paramater to achieve better results. If clustering takes too long, have a look at the parameters **n_init** and **max_iter** of ***KMeans(...)***.\n",
    "\n",
    "_***HINT:***_\n",
    "***KMeans(..)*** expects the features to be a stacked 2D matrix instead of a list (np.vstack(...)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSIFT Extraction: {0.8332625829998506}  seconds\n"
     ]
    }
   ],
   "source": [
    "#student code start\n",
    "\n",
    "# (For train images:)\n",
    "# Compute descriptors (dense-sift) for grid with stepsize 5\n",
    "train_descriptors_1 = np.array(features.extract_dsift(train_images, stepsize=5, num_samples=100))\n",
    "\n",
    "# Find clusters and cluster centers\n",
    "train_clusters = KMeans(n_clusters=50).fit(np.vstack(train_descriptors_1)).cluster_centers_\n",
    "\n",
    "#student code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3 Classification\n",
    "\n",
    "Generate a histogram per image using the centroids. First extract the dense SIFT features of the training set again. This time use a smaller stepsize (eg. 1 or 2) and take all samples.\n",
    "\n",
    "After, implement ***count_visual_words(..)*** in ***features.py*** to obtain a histogram per image. Build a kNN classifier with ***KNeighborsClassifier(..)*** and fit the the model with the histogram data of the training images and training labels. Set the neighbourhood size to **3**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student code start\n",
    "\n",
    "# (For train images:)\n",
    "# Compute descriptors (dense-sift) for grid with stepsize 2\n",
    "train_descriptors_2 = np.array(features.extract_dsift(train_images, stepsize=2))\n",
    "# Generate histograms\n",
    "train_histograms = features.count_visual_words(train_descriptors_2, train_clusters)\n",
    "\n",
    "# Fit histograms to classes (to annotated data)\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(train_histograms, train_dataset.get_data()[1])\n",
    "\n",
    "#student code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Evaluation\n",
    "\n",
    "Examine the accuracy of your model by extracting the dense SIFT points of the test set with the same stepsize as before, generate the histogams and predict the labels of the test set with the previously fitted classifier. Plot your results with ***utils.plot_confusion_matrix(..)***.\n",
    "\n",
    "_***HINT:***_\n",
    "Use the methods ***predict(..)*** and ***score(..)*** on the KNN classifier.\n",
    "\n",
    "\n",
    "***Submission:*** Save the confusion matrix with the KNN score as **task3_accuracy_test_data.png** using ***utils.plot_confusion_matrix(..)***.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student code start\n",
    "\n",
    "# (For test images:)\n",
    "# Compute descriptors (dense-sift) for grid with stepsize 2\n",
    "test_descriptors = np.array(features.extract_dsift(test_images, stepsize=2))\n",
    "# Generate histograms\n",
    "test_histograms = features.count_visual_words(test_descriptors, train_clusters)\n",
    "\n",
    "# Get predictions for histograms and predictions scores (mean accuracy)\n",
    "test_label_predictions = neigh.predict(test_histograms)\n",
    "test_label_scores = neigh.score(test_histograms, test_dataset.get_data()[1])\n",
    "\n",
    "utils.plot_confusion_matrix(test_dataset.get_data()[1], test_label_predictions, test_label_scores, class_names)\n",
    "\n",
    "#student code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Q1: Analyze the confusion matrix. Are there classes which can be more easily identified than others. What could be the reason? What classes perform worst, and explain your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forest, Coast and Tallbuilding are recognized the best, we suppose because they have unique visual-words (e.g. leafs, sea/horizon, windows). Classes which are semantically close (e.g. indoor) like bedroom, livingroom and kitchen share alot of common visual-words (e.g. chairs, tables, fabrics).\n",
    "\n",
    "# !!! TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Take some own test images and investigate if they can be correctly classified with you trained model. For instance, take a photo of your own kitchen or living room and classify it. You should at least test one photo from three of the ten categories. Make sure the images are resized and cropped to 100x100.\n",
    "\n",
    "***Submission:*** Save the confusion matrix with the model score as **task3_accuracy_custom_data.png** using ***utils.plot_confusion_matrix(..)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s7/lltj99px40l303qnnnpz1dsh0000gn/T/ipykernel_99479/2280198015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m\"./data/custom/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_dataset_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSceneDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest_images_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclass_names_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#student_code start\n",
    "\n",
    "# !!! TODO\n",
    "\n",
    "'''\n",
    "import cv2\n",
    "\n",
    "# Load test dataset\n",
    "test_path =  \"./data/custom/\"\n",
    "test_dataset_o = dataset.SceneDataset(test_path)\n",
    "test_images_o = test_dataset.get_data()[0]\n",
    "class_names_o = test_dataset_o.get_class_names()\n",
    "\n",
    "# Make images square and grayscale\n",
    "square_images_o = []\n",
    "for image in test_images_o:\n",
    "    h, w = image.shape\n",
    "    s = min(h, w)\n",
    "    crop_img = image[0:s, 0:s]\n",
    "    crop_img = cv2.resize(crop_img, (100, 100), interpolation = cv2.INTER_AREA)\n",
    "    square_images_o.append(crop_img)\n",
    "    \n",
    "test_descriptors_o = np.array(features.extract_dsift(square_images_o, stepsize=2))\n",
    "test_histograms_o = features.count_visual_words(test_descriptors_o, train_clusters)\n",
    "\n",
    "test_label_predictions_o = neigh.predict(test_histograms_o)\n",
    "test_label_scores_o = neigh.score(test_histograms_o, test_dataset_o.get_data()[1])\n",
    "\n",
    "utils.plot_confusion_matrix(test_dataset_o.get_data()[1], test_label_predictions_o, test_label_scores_o, class_names_o)\n",
    "'''\n",
    "\n",
    "#student code end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
